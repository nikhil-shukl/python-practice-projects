{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c238f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file created!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    \"label\": [\"ham\", \"spam\", \"ham\", \"spam\", \"ham\"],\n",
    "    \"message\": [\n",
    "        \"Hey are we meeting today\",\n",
    "        \"Win cash now\",\n",
    "        \"Call me when you are free\",\n",
    "        \"Free entry in contest text WIN\",\n",
    "        \"Lets have dinner tonight\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(\"spam_data.csv\", index=False)\n",
    "\n",
    "print(\"CSV file created!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65940d6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  label                         message\n",
      "0   ham        Hey are we meeting today\n",
      "1  spam                    Win cash now\n",
      "2   ham       Call me when you are free\n",
      "3  spam  Free entry in contest text WIN\n",
      "4   ham        Lets have dinner tonight\n"
     ]
    }
   ],
   "source": [
    "messages = pd.read_csv(\"spam_data.csv\")\n",
    "print(messages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "121109cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Nikki\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Data Cleaning And Preprocessing\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b75c597d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "ps=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "577f0278",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=[]\n",
    "for i in range(0,len(messages)):\n",
    "    review=re.sub('[^a-zA-z]',' ',messages['message'][i])\n",
    "    review=review.lower()\n",
    "    review=review.split()\n",
    "    review=[ps.stem(word) for word in review if not word in stopwords.words('english')]\n",
    "    review=' '.join(review)\n",
    "    corpus.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42ad3308",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hey meet today',\n",
       " 'win cash',\n",
       " 'call free',\n",
       " 'free entri contest text win',\n",
       " 'let dinner tonight']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc03be9d",
   "metadata": {},
   "source": [
    "## Create Bag Of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69168eda",
   "metadata": {},
   "source": [
    "scikit-learn = Python ki ML library\n",
    "\n",
    "ML ke liye ready-made tools deta hai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ecfa57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create the Bag OF Words model\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "## for Binary BOW enable binary=True\n",
    "cv=CountVectorizer(max_features=100,binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "edef63d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=cv.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07f2807f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "       [1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(edgeitems=30, linewidth=100000, \n",
    "    formatter=dict(float=lambda x: \"%.3g\" % x))\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75246fbe",
   "metadata": {},
   "source": [
    "### N-Grams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b74db4",
   "metadata": {},
   "source": [
    "N-grams convert text into binary vectors using word combinations, allowing models to capture context such as negation (not good) which unigram Bag of Words fails to capture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76183dd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hey': np.int64(6),\n",
       " 'meet': np.int64(8),\n",
       " 'today': np.int64(10),\n",
       " 'win': np.int64(12),\n",
       " 'cash': np.int64(1),\n",
       " 'call': np.int64(0),\n",
       " 'free': np.int64(5),\n",
       " 'entri': np.int64(4),\n",
       " 'contest': np.int64(2),\n",
       " 'text': np.int64(9),\n",
       " 'let': np.int64(7),\n",
       " 'dinner': np.int64(3),\n",
       " 'tonight': np.int64(11)}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cff4c1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create the Bag OF Words model with ngram\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "## for Binary BOW enable binary=True\n",
    "cv=CountVectorizer(max_features=100,binary=True,ngram_range=(2,3))\n",
    "X=cv.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "18650ce4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hey meet': np.int64(8),\n",
       " 'meet today': np.int64(12),\n",
       " 'hey meet today': np.int64(9),\n",
       " 'win cash': np.int64(14),\n",
       " 'call free': np.int64(0),\n",
       " 'free entri': np.int64(6),\n",
       " 'entri contest': np.int64(4),\n",
       " 'contest text': np.int64(1),\n",
       " 'text win': np.int64(13),\n",
       " 'free entri contest': np.int64(7),\n",
       " 'entri contest text': np.int64(5),\n",
       " 'contest text win': np.int64(2),\n",
       " 'let dinner': np.int64(10),\n",
       " 'dinner tonight': np.int64(3),\n",
       " 'let dinner tonight': np.int64(11)}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd7c5e5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d510f0",
   "metadata": {},
   "source": [
    "## Create TF-IDF And NGrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "be77ad29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fd41622e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf=TfidfVectorizer(max_features=100)\n",
    "X=tfidf.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4d51d01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(edgeitems=30, linewidth=100000, \n",
    "    formatter=dict(float=lambda x: \"%.3g\" % x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "239b3bb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0.577, 0, 0.577, 0, 0.577, 0, 0],\n",
       "       [0, 0.778, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.628],\n",
       "       [0.778, 0, 0, 0, 0, 0.628, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0.482, 0, 0.482, 0.389, 0, 0, 0, 0.482, 0, 0, 0.389],\n",
       "       [0, 0, 0, 0.577, 0, 0, 0, 0.577, 0, 0, 0, 0.577, 0]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249c7254",
   "metadata": {},
   "source": [
    "TF-IDF captures word importance but not word relationships.\n",
    "N-grams are required to capture contextual meaning like negation.\n",
    "Hence, TF-IDF with N-grams is preferred in sentiment analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e8d895",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testingopenai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
